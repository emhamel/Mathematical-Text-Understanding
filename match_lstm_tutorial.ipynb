{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "080a2b73",
   "metadata": {},
   "source": [
    "# Mathematical Language Text Understanding with Match-LSTM Network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5466928a",
   "metadata": {},
   "source": [
    "In this tutorial, we will be using a Match-LSTM model to predict the position of a definiton of a mathematical variable given a variable name and the context in which it is defined.\n",
    "\n",
    "# The Match-LSTM Model\n",
    "\n",
    "This model combines multiple attention mechanisms, like Bahdanau-like attention and pointer networks to produce an output. Broadly, the model functions as follows: the model is given a variable name and the context in which it is defined. Then, attention scores for each variable name-context word pair will be computed and processed in an LSTM. The hidden states for this layer are passed to a pointer network that will output the most likely positional of the definition. The following figure shows how each attention layer stacks together to produce the final output.\n",
    "\n",
    "<div>\n",
    "<img src=\"images/match-lstm.jpg\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "Let's take a better look at how this model functions. First, the model calulates the hidden states for the context text and the variable name. Then, the attention weights are calculated for every context-variable hidden state pair, which is then feed into a bi-directional LSTM (referred to as a match-LSTM). The attention in the match-LSTM layer is calculated as such:\n",
    "\n",
    "\\begin{align}\n",
    "G_{i} = tanh(W^{v}h^{v} + (W^{c}h_{i}^{c} + W^{r}h_{i-1}^{r} + b^{c}))\n",
    "\\end{align}\n",
    "\\begin{align}\n",
    "\\alpha_{i} = w^{\\top}G_{i} + b\n",
    "\\end{align}\n",
    "\n",
    "Here, $W^{v}, W^{c}, W^{r}, b^{c}, w, b$ are learned weights, $h_{i}^{c}$ and $h^{v}$ are the hidden states of the $i-th$ word of the context text and the variable name respectively, and $h_{i-1}^{r}$ is the previous hidden state of the match-lstm. The attention weights are then combine with $h_{i}^{c}$ and $h^{v}$ and processed in the match-LSTM. This process is done in the forward and backwards direction and all resulting hidden states are coalesced into a final hidden state vector. The final hidden state vector is processed by a pointer layer. This layer then calculates the most likely position of the definition of variable in the context text. The following formulas show how this is done:\n",
    "\n",
    "\\begin{align}\n",
    "F_{j} = tanh(V\\bar{H^{r}} + (W^{a}h_{k - 1}^{a} + b^{a}) \\otimes e_{C + 1})\n",
    "\\end{align}\n",
    "\\begin{align}\n",
    "\\beta_{j} = softmax(v^{\\top}F_{j} + b \\otimes e_{C + 1}) \n",
    "\\end{align}\n",
    "\n",
    "where $V, W^{a}, b^{a}, v$ and $b$ are learned weights, $\\bar{H^{r}}$ is the vector of concatenated match-LSTM hidden states, $h_{k - 1}^{a}$ is the previous hidden state of the pointer LSTM and $e_{C + 1}$ is a vector of ones with size C + 1, where C is the length of the context text. $\\beta_{j}$ is combined with $h_{j}^{c}$ and processed by an LSTM.\n",
    "\n",
    "Our model is a slight modification of the model used descibed in this paper: https://arxiv.org/pdf/1608.07905.pdf. We removed the softmax from the attention scores in the Match-LSTM layer as the variable names in our data are always one word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1d3612",
   "metadata": {},
   "source": [
    "## Let's get started!\n",
    "To start, let's define a Pytorch Dataset. This is a data object will take our data and prepare it to be fed into a model. This entails cleaning the data, tokenizing it, and partitioning it into batchs. I highly recommend the use of the pytorch dataset object in any pytorch project you may do because of its compatability with the pytorch dataloader. The dataloader will batch and randomize your data for you, making it easy to train your model.\n",
    "\n",
    "## The Data and How We Process It\n",
    "The data used in this tutorial is sourced from mathematical and physics papers from arXiv and manually cleaned. For each entry in the dataset, we are given a variable name, the context in which it is defined, and an definition of the variable. This outputs the most likely position of the definition in the context text, so all output tensors represent the indexes of the definition. \n",
    "\n",
    "Our dataset has variable length definitions, so we need a way to adding an \\<EOS\\> and \\<PAD\\> token to output tensors. To do this, we add an \\<EOS\\> and \\<PAD\\> token to the end of each context so the output can use their positions to generate those tokens. We also pad each output to the maximum output length for ease of training and testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25970325",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57a18ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatchLSTMDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        \n",
    "        # The tokenized inputs and outputs\n",
    "        self.vars = []\n",
    "        self.contexts = []\n",
    "        self.outputs = []\n",
    "\n",
    "        # Maps each index to a unique word and vice versa\n",
    "        self.word2index = {}\n",
    "        self.index2word = {0: \"PAD\", 1: \"EOS\"}\n",
    "        \n",
    "        self.pad = 0\n",
    "        self.eos = 1\n",
    "        self.size = len(self.index2word)\n",
    "        self.context_max_len = 0\n",
    "        self.def_max_len = 0\n",
    "        \n",
    "        # Process the data\n",
    "        self.prep_data()\n",
    "\n",
    "    def add_text(self, text):\n",
    "        \"\"\"Assigns a unique index to each word in a sentence.\n",
    "\n",
    "        Args:\n",
    "          text: A sentence\n",
    "        \"\"\"\n",
    "        for word in text.split(' '):\n",
    "            if word not in self.word2index:\n",
    "                self.word2index[word] = self.size\n",
    "                self.index2word[self.size] = word\n",
    "                self.size += 1\n",
    "\n",
    "    def clean_text(self, string):\n",
    "        \"\"\"Strips text of any punctuation, converts to lower case, and gets rid of any \n",
    "        unnecessary white space.\n",
    "\n",
    "        Args:\n",
    "          text: the uncleaned sentence.\n",
    "\n",
    "        Returns:\n",
    "          The cleaned sentence.\n",
    "        \"\"\"\n",
    "        string = string.replace('\"', '').lower().strip()\n",
    "        string = string.replace(\",\", \"\").replace(\".\", \"\").replace(\"!\", \"\").replace(\"?\", \"\").replace(\"(\", \"\").replace(\")\", \"\").replace(\":\", \"\").replace(\"|\", \" \").replace(\"  \", \" \").strip().replace(\";\", \"\")\n",
    "        string = string.split(\" \")\n",
    "        string = ' '.join(string)\n",
    "        return string\n",
    "\n",
    "    def get_idx(self, context, definition):\n",
    "        \"\"\"This function finds positional token of 'definition' in 'context'. \n",
    "\n",
    "        Args:\n",
    "          context: the context in which the variable is defined\n",
    "          definition: the definition of the variable\n",
    "\n",
    "        Returns:\n",
    "          The position of 'definition' in 'context'\n",
    "        \"\"\"\n",
    "        context_split = context.split(\" \")\n",
    "        if definition in context_split:\n",
    "            # base case, return the idx of the definition in the context\n",
    "            idx = context_split.index(definition)\n",
    "        else:\n",
    "            # This should cover an edge case where the correct definition is the singular version\n",
    "            # of the definition of the text. It finds the idx of the most similar word to the definition. \n",
    "            max_common = 0\n",
    "            most_likely_idx = 0\n",
    "            for i in range(len(context_split)):\n",
    "                word_dist = len(set(definition).intersection(set(context_split[i])))\n",
    "                if max_common < word_dist:\n",
    "                    max_common = word_dist\n",
    "                    most_likely_idx = i\n",
    "                        \n",
    "            idx = most_likely_idx\n",
    "        \n",
    "        return idx\n",
    "\n",
    "    def prep_data(self):\n",
    "        \"\"\"The main function of the dataset object.\n",
    "        This function cleans and tokenizes each entry, as well as set the maximum text and definition length. \n",
    "        \"\"\"\n",
    "        context_max_len = 0\n",
    "        def_max_len = 0\n",
    "        for index, row in self.df.iterrows():\n",
    "            var = self.clean_text(row[\"var\"])\n",
    "            context = self.clean_text(row[\"context\"])\n",
    "            definition = self.clean_text(row[\"def\"])\n",
    "\n",
    "            self.add_text(context)\n",
    "            \n",
    "            # Add an EOS and a PAD token so the pointer layer can generate those tokens\n",
    "            # This way, out pointer layer can handle variable length outputs\n",
    "            context_text = [self.word2index[c] for c in context.split(\" \")] + [self.eos] + [self.pad]\n",
    "            \n",
    "            # Output tensors are the positions of the words in the definitions in the context\n",
    "            output_text = [self.get_idx(context, word) for word in definition.split(\" \")]\n",
    "            \n",
    "            self.vars.append([self.word2index[var]])\n",
    "            self.contexts.append(context_text)\n",
    "            self.outputs.append(output_text)\n",
    "\n",
    "            if len(context_text) > context_max_len:\n",
    "                context_max_len = len(context_text)\n",
    "\n",
    "            if len(output_text) > def_max_len:\n",
    "                def_max_len = len(output_text)\n",
    "\n",
    "        # Set the maximum text and definition length\n",
    "        # This is important when defining the models\n",
    "        self.context_max_len = context_max_len\n",
    "        self.def_max_len = def_max_len + 1\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        \"\"\"This function is used by the dataloader to create batchs.\n",
    "        It seperated the inputs and the outputs and then pads them.\n",
    "\n",
    "        Args:\n",
    "          batch: A list of data entries of size batch_size.\n",
    "\n",
    "        Returns:\n",
    "          The padded inputs and outputs.\n",
    "        \"\"\"\n",
    "        variables = []\n",
    "        contexts = []\n",
    "        defs = []\n",
    "        \n",
    "        max_output_len = max([len(b[2]) for b in batch])\n",
    "        \n",
    "        for b in batch:\n",
    "            variables.append(torch.LongTensor(b[0]))\n",
    "            contexts.append(torch.LongTensor(b[1]))\n",
    "            \n",
    "            # all outputs are padded to the maximum output length\n",
    "            output_pad_length = self.def_max_len - len(b[2]) - 1\n",
    "            \n",
    "            # each context text has an EOS token and a PAD token at the end\n",
    "            # The pointer network points to these tokens when generating an EOS or a PAD token\n",
    "            output_eos = len(b[1]) - 2\n",
    "            output_pad = len(b[1]) - 1\n",
    "            defs.append(torch.LongTensor(b[2] + [output_eos] + ([output_pad] * output_pad_length)))\n",
    "\n",
    "        return (pad_sequence(variables), pad_sequence(contexts), pad_sequence(defs))\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"This is a required function.\n",
    "        It returns the size of the dataset.\n",
    "\n",
    "        Returns:\n",
    "          The size of the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.contexts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"This is a required function.\n",
    "        It returns the data entry at index at idx.\n",
    "\n",
    "        Args:\n",
    "          idx: An index.\n",
    "\n",
    "        Returns:\n",
    "          The data entry at index idx.\n",
    "        \"\"\"\n",
    "        return (self.vars[idx], self.contexts[idx], self.outputs[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db9bae6",
   "metadata": {},
   "source": [
    "## Implementing the Match-LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ef63f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatchLSTM(nn.Module):\n",
    "    \"\"\" This class is the second layer of the model. It calculates attention between the \n",
    "    variable name and the context tokens using an LSTM. \n",
    "    \"\"\"\n",
    "    def __init__(self, context_max_len, def_max_len, hidden_size, batch_size, device):\n",
    "        super().__init__()\n",
    "        self.context_max_len = context_max_len\n",
    "        self.def_max_len = def_max_len\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "\n",
    "        self.context_weights = nn.Linear(hidden_size, hidden_size)\n",
    "        self.var_weights = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.prev_weights = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.alpha_weights = nn.Linear(hidden_size, 1, bias=False)\n",
    "        self.b = nn.Parameter(torch.empty(1))\n",
    "\n",
    "        self.lstm_match = nn.LSTMCell(2 * hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, var_hidden, context_hidden):\n",
    "        context_size = context_hidden.size(0)\n",
    "\n",
    "        h_forward = torch.zeros((self.context_max_len, self.batch_size, self.hidden_size), device=self.device)\n",
    "        h_back = torch.zeros((self.context_max_len, self.batch_size, self.hidden_size), device=self.device)\n",
    "\n",
    "        h_prev = torch.zeros(self.batch_size, self.hidden_size, device=self.device)\n",
    "        c = torch.zeros(self.batch_size, self.hidden_size, device=self.device)\n",
    "\n",
    "        # forward pass\n",
    "        for i in range(context_size):\n",
    "            weighted_var = self.var_weights(var_hidden)\n",
    "            weighted_context = self.context_weights(context_hidden[i]) + self.prev_weights(h_prev) # (batch_size, hidden_size)\n",
    "\n",
    "            G = torch.tanh(weighted_var + weighted_context)\n",
    "            \n",
    "            # use softmax if len(var name) > 1\n",
    "            a = self.alpha_weights(G) + self.b # (batch_size, 1)\n",
    "            \n",
    "            z = torch.cat((context_hidden[i], torch.bmm(var_hidden.unsqueeze(2), a.unsqueeze(1)).squeeze(2)), dim=1) # (batch_size, hidden_size * 2)\n",
    "            h_prev, c = self.lstm_match(z, (h_prev, c))\n",
    "            h_forward[i] = h_prev\n",
    "\n",
    "        h_prev = torch.zeros(batch_size, self.hidden_size, device=self.device)\n",
    "        c = torch.zeros(batch_size, self.hidden_size, device=self.device)\n",
    "        \n",
    "        # backwards pass\n",
    "        for j in range(context_size):\n",
    "            idx = context_size - 1 - j\n",
    "            weighted_var = self.var_weights(var_hidden)\n",
    "            weighted_context = self.context_weights(context_hidden[idx]) + self.prev_weights(h_prev) # (batch_size, hidden_size)\n",
    "\n",
    "            G = torch.tanh(weighted_var + weighted_context)\n",
    "            \n",
    "            # use softmax if len(var name) > 1\n",
    "            a = self.alpha_weights(G) + self.b # (batch_size, 1)\n",
    "\n",
    "            z = torch.cat((context_hidden[idx], torch.bmm(var_hidden.unsqueeze(2), a.unsqueeze(1)).squeeze(2)), dim=1) # (batch_size, hidden_size * 2)\n",
    "            h_prev, c = self.lstm_match(z, (h_prev, c))\n",
    "            h_back[idx] = h_prev\n",
    "\n",
    "        h_forward = h_forward.permute((1, 0, 2)) # (batch_size, context_max_len, hidden_size)\n",
    "        h_back = h_back.permute((1, 0, 2)) # (batch_size, context_max_len, hidden_size)\n",
    "\n",
    "        return torch.cat((h_forward, h_back), dim=2) # (batch_size, context_max_len, hidden_size * 2)\n",
    "\n",
    "class PointerLayer(nn.Module):\n",
    "    \"\"\"This class is the last layer of the model. It takes the attention calculated by the\n",
    "    MatchLSTM layer and decides which tokens in the context are the definition. In this tutorial,\n",
    "    we used a 'sequence' pointer network as it is better suited for this task then the boundary network. \n",
    "    \"\"\"\n",
    "    def __init__(self, context_max_len, def_max_len, hidden_size, batch_size, device):\n",
    "        super().__init__()\n",
    "        self.context_max_len = context_max_len\n",
    "        self.def_max_len = def_max_len\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "\n",
    "        self.context_weights = nn.Linear(2 * hidden_size, hidden_size, bias=False)\n",
    "        self.prev_weights = nn.Linear(hidden_size, hidden_size)\n",
    "        self.v = nn.Linear(hidden_size, 1, bias=False)\n",
    "        self.c = nn.Parameter(torch.empty(1))\n",
    "        self.lstm_cell = nn.LSTMCell(2 * hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, H):\n",
    "        h_prev = torch.zeros(self.batch_size, self.hidden_size, device=self.device)\n",
    "        c = torch.zeros(self.batch_size, self.hidden_size, device=self.device)\n",
    "\n",
    "        probs = []\n",
    "        for i in range(self.def_max_len):\n",
    "            prev = self.prev_weights(h_prev).unsqueeze(1).repeat(1, self.context_max_len, 1) # (batch_size, context_max_len, hidden size * 2)\n",
    "            f = torch.tanh(self.context_weights(H) + prev) # (batch_size, context_max_len, hidden size * 2)\n",
    "            beta = self.v(f) + self.c\n",
    "            beta = torch.permute(beta, (0, 2, 1))\n",
    "            beta = F.log_softmax(beta, dim=2) # (batch_size, 1, context_max_len)\n",
    "\n",
    "            attn_applied = torch.bmm(torch.permute(H, (0, 2, 1)), torch.transpose(beta, dim0=1, dim1=2)).squeeze(-1) # (batch_size, hidden_size * 2)\n",
    "            h_prev, c = self.lstm_cell(attn_applied, (h_prev, c))\n",
    "            probs.append(beta.squeeze(1))\n",
    "\n",
    "        probs = torch.stack(probs, dim=1) # (batch_size, def_max_len, context_max_len)\n",
    "        return probs\n",
    "\n",
    "class MatchLSTMModel(nn.Module):\n",
    "    def __init__(self, learning_rate, context_max_len, def_max_len, size, hidden_size, optimizer, batch_size):\n",
    "        super().__init__()\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.embed = nn.Embedding(size, hidden_size)\n",
    "\n",
    "        self.context_encoder = nn.LSTM(hidden_size, hidden_size)\n",
    "        self.def_encoder = nn.LSTM(hidden_size, hidden_size)\n",
    "\n",
    "        self.match_layer = MatchLSTM(context_max_len, def_max_len, hidden_size, batch_size, device)\n",
    "        self.pointer_layer = PointerLayer(context_max_len, def_max_len, hidden_size, batch_size, device)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.context_max_len = context_max_len\n",
    "        self.def_max_len = def_max_len\n",
    "\n",
    "        if optimizer == \"sgd\":\n",
    "            self.context_opt = optim.SGD(self.context_encoder.parameters(), lr=learning_rate)\n",
    "            self.var_opt = optim.SGD(self.def_encoder.parameters(), lr=learning_rate)\n",
    "            self.match_opt = optim.SGD(self.match_layer.parameters(), lr=learning_rate)\n",
    "            self.pointer_opt = optim.SGD(self.pointer_layer.parameters(), lr=learning_rate)\n",
    "        else:\n",
    "            self.context_opt = optim.Adam(self.context_encoder.parameters(), lr=learning_rate)\n",
    "            self.var_opt = optim.Adam(self.def_encoder.parameters(), lr=learning_rate)\n",
    "            self.match_opt = optim.Adam(self.match_layer.parameters(), lr=learning_rate)\n",
    "            self.pointer_opt = optim.Adam(self.pointer_layer.parameters(), lr=learning_rate)\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, variable, context):\n",
    "        var_embed = self.embed(variable)\n",
    "        context_embed = self.embed(context)\n",
    "        \n",
    "        var_hidden, (_, _) = self.def_encoder(var_embed)\n",
    "        var_hidden = var_hidden.squeeze(0) # (batch_size, hidden_size)\n",
    "        \n",
    "        context_hidden_states, (_, _) = self.context_encoder(context_embed) # (context_length, batch_size, hidden_size)\n",
    "\n",
    "        match_states = self.match_layer(var_hidden, context_hidden_states) # (batch_size, context_max_len, hidden_size * 2)\n",
    "        probs = self.pointer_layer(match_states) # (batch_size, def_max_len, context_max_len + 1)\n",
    "        return probs\n",
    "\n",
    "    def update_opts(self, loss):\n",
    "        \"\"\"This function applies the loss each layer of the model.\n",
    "\n",
    "        Args:\n",
    "          loss: the loss to update the layers.\n",
    "\n",
    "        \"\"\"\n",
    "        self.context_opt.zero_grad()\n",
    "        self.var_opt.zero_grad()\n",
    "        self.match_opt.zero_grad()\n",
    "        self.pointer_opt.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        self.context_opt.step()\n",
    "        self.var_opt.step()\n",
    "        self.match_opt.step()\n",
    "        self.pointer_opt.step()\n",
    "\n",
    "    def train(self, dataloader, epochs):\n",
    "        for e in range(epochs):\n",
    "            avg_loss = 0\n",
    "            # var: (1, batch_size), context: (context_len, batch_size), definition: (def_size, batch_size)\n",
    "            for var, context, definition in dataloader:\n",
    "                probs = self.forward(var, context)\n",
    "                outputs = probs.permute((1, 0, 2)) # (def_max_len, batch_size, context_max_len)\n",
    "                \n",
    "                loss = 0\n",
    "                for i in range(self.def_max_len):\n",
    "                    loss += F.nll_loss(outputs[i], definition[i])\n",
    "                \n",
    "                self.update_opts(loss)\n",
    "                avg_loss += loss.item()\n",
    "\n",
    "            if e % 2 == 0:\n",
    "                print(\"epoch\", e, np.round(avg_loss / len(dataloader), 3))\n",
    "\n",
    "    def evaluate(self, dataloader):\n",
    "        correct = 0\n",
    "        precision = 0\n",
    "        recall = 0\n",
    "\n",
    "        # var: (1, batch_size), context: (context_len, batch_size), definition: (def_size, batch_size)\n",
    "        for var, context, definition in dataloader:\n",
    "            with torch.no_grad():\n",
    "                outputs = self.forward(var, context)\n",
    "                outputs = torch.argmax(outputs, -1)\n",
    "                \n",
    "                # permute so every tensor is one output\n",
    "                target_tensor = definition.permute((1, 0)).type(torch.LongTensor)\n",
    "\n",
    "                # Count the number of completely correct outputs\n",
    "                correct += sum([1 for i, j in zip(outputs, target_tensor) if torch.equal(i, j)])             \n",
    "\n",
    "                precision += sum([len(np.intersect1d(i, j)) / len(i) for i, j in zip(outputs, target_tensor)])\n",
    "                recall += sum([len(np.intersect1d(i, j)) / len(j) for i, j in zip(outputs, target_tensor)])\n",
    "\n",
    "        data_size = (dataloader.batch_size * len(dataloader))\n",
    "        \n",
    "        # Get average precision and recall score\n",
    "        precision /= data_size\n",
    "        recall /= data_size\n",
    "\n",
    "        F1_score = (2 * precision * recall) / (precision + recall)\n",
    "        return np.round(correct / data_size, 3), np.round(F1_score, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43460e4f",
   "metadata": {},
   "source": [
    "## Putting It All Together\n",
    "Now that we have all the important parts implemented, let's train and evaluate the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ef097f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data.csv\")\n",
    "\n",
    "# Randomize the dataframe\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "dataset = MatchLSTMDataset(data)\n",
    "\n",
    "# we use a 80/20 split to partition the data into training and testing data\n",
    "i = math.floor(0.8 * len(data))\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [i, len(dataset) - i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e7bb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play around with these variables!\n",
    "hid_size = 256 # hidden state size\n",
    "lr = 0.1\n",
    "epochs = 5\n",
    "optimizer = 'sgd' # can also be 'adam'\n",
    "batch_size = 16\n",
    "\n",
    "# Dataloaders are awesome: they randomize and batch your data for you!\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, collate_fn=dataset.collate_fn, drop_last=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=0, collate_fn=dataset.collate_fn, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9efde9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MatchLSTMModel(lr, dataset.context_max_len, dataset.def_max_len, dataset.size, hid_size, optimizer, batch_size)\n",
    "print(\"TRAINING MODEL\")\n",
    "model.train(train_dataloader, epochs)\n",
    "\n",
    "# Evaluate model\n",
    "training_total_correct, training_F1_score = model.evaluate(train_dataloader)\n",
    "print(\"PERFORMANCE ON TRAINING DATA\", \"completely correct:\", training_total_correct, \"F1 score:\", training_F1_score)\n",
    "\n",
    "testing_total_correct, testing_F1_score = model.evaluate(test_dataloader)\n",
    "print(\"PERFORMANCE ON TESTING DATA\", \"completely correct:\", testing_total_correct, \"F1 score:\", testing_F1_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
